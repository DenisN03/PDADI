{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Формирование датасета для обучения НС\n",
    "Подготовка данных для обучения. На этом этапе выполняется формирование набора обрезанных изображений с детекциями людей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import tqdm\n",
    "\n",
    "sys.path.append('/app/src')\n",
    "import glob\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import yaml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from markup_convertors import yolo2bb\n",
    "from transforms import crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 58/58 [00:56<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Обрезка\n",
    "\n",
    "DATASET_VERSION = \"v8\"\n",
    "\n",
    "people_img_path = \"/app/data/interim_v3/images/train/\"\n",
    "labels_img_path = \"/app/data/interim_v3/labels/train/\"\n",
    "\n",
    "people_save_path = f\"/app/data/processed/{DATASET_VERSION}/images_{DATASET_VERSION}/\"\n",
    "labels_save_path = f\"/app/data/processed/{DATASET_VERSION}/labels_{DATASET_VERSION}/\"\n",
    "\n",
    "people_back_save_path = f\"/app/data/processed/{DATASET_VERSION}/images_back_{DATASET_VERSION}/\"\n",
    "labels_back_save_path = f\"/app/data/processed/{DATASET_VERSION}/labels_back_{DATASET_VERSION}/\"\n",
    "\n",
    "COLS = 4\n",
    "ROWS = 3\n",
    "\n",
    "Path(people_save_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(labels_save_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(people_back_save_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(labels_back_save_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for img in tqdm.tqdm(glob.glob(people_img_path + \"*\")):\n",
    "    \n",
    "    image = cv2.imread(img)\n",
    "    \n",
    "    height, width, ch = image.shape\n",
    "    W_STEP = width / COLS\n",
    "    H_STEP = height / ROWS\n",
    "    \n",
    "    filename = os.path.splitext(os.path.basename(img))[0]\n",
    "    \n",
    "    file = open(labels_img_path + filename + \".txt\", \"r\")\n",
    "    markup = []\n",
    "    for line in file.readlines():\n",
    "        markup.append(yolo2bb(line, width, height))\n",
    "    \n",
    "    # for bbox in markup:\n",
    "    #     cv2.rectangle(image,(bbox[1],bbox[2]),(bbox[3],bbox[4]),(0,255,0), 3)\n",
    "    #     cv2.imwrite(f'{people_save_path}/{filename}.jpg', image)\n",
    "    \n",
    "    # Base crop\n",
    "    crop(image, ROWS, COLS, H_STEP, W_STEP, \"B\", people_save_path, people_back_save_path, markup, filename)\n",
    "    # Support crop 1\n",
    "    crop(image, ROWS, COLS-1, H_STEP, W_STEP, \"C\", people_save_path, people_back_save_path, markup, filename)\n",
    "    # Support crop 2\n",
    "    crop(image, ROWS-1, COLS, H_STEP, W_STEP, \"R\", people_save_path, people_back_save_path, markup, filename)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделение данных на обучающую и тестовую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_VERSION = \"v8\"\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "dataset_path = f\"/app/data/processed/{DATASET_VERSION}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Общее количество изображений: 171\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(dataset_path + f'images_{DATASET_VERSION}')\n",
    "print(f'Общее количество изображений: {len(files)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rm6f742rCfPp",
    "outputId": "d64b3642-68f5-4671-d715-2922f4a22ab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество изображений для обучения: 136\n",
      "Количество изображений для тестирования: 35\n"
     ]
    }
   ],
   "source": [
    "train_images_names, test_images_names = train_test_split(files, test_size=TEST_SIZE, random_state=22)\n",
    "print(f'Количество изображений для обучения: {len(train_images_names)}')\n",
    "print(f'Количество изображений для тестирования: {len(test_images_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(images, ds_type):\n",
    "    Path(os.path.join(dataset_path, f'images/{ds_type}/')).mkdir(parents=True, exist_ok=True)\n",
    "    Path(os.path.join(dataset_path, f'labels/{ds_type}/')).mkdir(parents=True, exist_ok=True)\n",
    "    for image in images:\n",
    "        shutil.copy(os.path.join(dataset_path, f'images_{DATASET_VERSION}', image), os.path.join(dataset_path, f'images/{ds_type}/'))\n",
    "        shutil.copy(os.path.join(dataset_path, f'labels_{DATASET_VERSION}', image.split('.')[0]) + '.txt', os.path.join(dataset_path, f'labels/{ds_type}/'))\n",
    "        \n",
    "split(test_images_names, \"val\")\n",
    "split(train_images_names, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка количества данных для обучения и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gtqj-p75GcZ7",
    "outputId": "4ac36b4d-7d24-49b2-a604-c6489e0983a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные для обучения корректны.\n",
      "Данные для валидации корректны.\n"
     ]
    }
   ],
   "source": [
    "def check_splits(dataset_path):\n",
    "    images_train = len(os.listdir(dataset_path + 'images/train/'))\n",
    "    images_val = len(os.listdir(dataset_path + 'images/val/'))\n",
    "    labels_train = len(os.listdir(dataset_path + 'labels/train/'))\n",
    "    labels_val = len(os.listdir(dataset_path + 'labels/val/'))\n",
    "    if images_train != labels_train:\n",
    "        print(f'Разное количество данных для обучения: \\\n",
    "              количество изображений: {images_train} \\\n",
    "              количество файлов с разметкой: {labels_train}')\n",
    "    else:\n",
    "        print('Данные для обучения корректны.')\n",
    "    if images_val != labels_val:\n",
    "        print(f'Разное количество данных для валидации: \\\n",
    "              количество изображений: {images_val} \\\n",
    "              количество файлов с разметкой: {labels_val}')\n",
    "    else:\n",
    "        print('Данные для валидации корректны.')\n",
    "    \n",
    "\n",
    "check_splits(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавление в датасет изображений без разметки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_back_images(ds_type):\n",
    "    train_count = len(os.listdir(dataset_path + f'images/{ds_type}/'))\n",
    "    train_back_count = int(train_count * 0.1)\n",
    "    train_back_total = len(os.listdir(dataset_path + f'images_back_{DATASET_VERSION}'))\n",
    "    train_indexes = random.sample(range(1, train_back_total), train_back_count)\n",
    "\n",
    "    for i, train_back_image in enumerate(glob.glob(dataset_path + f'images_back_{DATASET_VERSION}/*')):\n",
    "        if i in train_indexes:\n",
    "            shutil.copy(train_back_image, os.path.join(dataset_path, f'images/{ds_type}/'))\n",
    "            shutil.copy(\".\".join(train_back_image.split('.')[:-1]).replace(\"images\",\"labels\") + '.txt', os.path.join(dataset_path, f'labels/{ds_type}/'))\n",
    "            \n",
    "add_back_images(\"train\")\n",
    "add_back_images(\"val\")\n",
    "\n",
    "check_splits(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Перемещение данных в папку для обчения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/app/data/train/v8/’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir /app/data/train/v8/\n",
    "! mv /app/data/processed/v8/images /app/data/train/v8/\n",
    "! mv /app/data/processed/v8/labels /app/data/train/v8/\n",
    "! chmod -R 777 /app/data/train/v8/\n",
    "! chmod -R 777 /app/data/processed/v8/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BJ-jh9jOB0H"
   },
   "source": [
    "## Обучение\n",
    "Обучение выполняется через bash скрипт `train.sh`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "LA9f-mEWOLkH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory yolov5 is not found. Downloading...\n",
      "Cloning into 'yolov5'...\n",
      "remote: Enumerating objects: 14765, done.\u001b[K\n",
      "remote: Counting objects: 100% (96/96), done.\u001b[K\n",
      "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
      "remote: Total 14765 (delta 53), reused 72 (delta 38), pack-reused 14669\u001b[K\n",
      "Receiving objects: 100% (14765/14765), 13.60 MiB | 6.51 MiB/s, done.\n",
      "Resolving deltas: 100% (10190/10190), done.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5x.pt, cfg=, data=/app/data/train/v8/dataset.yaml, hyp=/app/yolov5/data/hyps/hyp.scratch-low.yaml, epochs=100, batch_size=2, imgsz=320, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=/app/train_logs, name=yolov5x_dv8_tmp, exist_ok=True, quad=False, cos_lr=False, label_smoothing=0.0, patience=20, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov5 ✅\n",
      "YOLOv5 🚀 v6.2-237-g55e9516 Python-3.8.13 torch-1.13.0a0+d0d6b1f CUDA:0 (NVIDIA GeForce RTX 2060, 5935MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mrun 'pip install clearml' to automatically track, visualize and remotely train YOLOv5 🚀 in ClearML\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 🚀 runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /app/train_logs', view at http://localhost:6006/\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
      "100%|████████████████████████████████████████| 755k/755k [00:00<00:00, 3.13MB/s]\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.2/yolov5x.pt to yolov5x.pt...\n",
      "100%|████████████████████████████████████████| 166M/166M [00:13<00:00, 12.6MB/s]\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      8800  models.common.Conv                      [3, 80, 6, 2, 2]              \n",
      "  1                -1  1    115520  models.common.Conv                      [80, 160, 3, 2]               \n",
      "  2                -1  4    309120  models.common.C3                        [160, 160, 4]                 \n",
      "  3                -1  1    461440  models.common.Conv                      [160, 320, 3, 2]              \n",
      "  4                -1  8   2259200  models.common.C3                        [320, 320, 8]                 \n",
      "  5                -1  1   1844480  models.common.Conv                      [320, 640, 3, 2]              \n",
      "  6                -1 12  13125120  models.common.C3                        [640, 640, 12]                \n",
      "  7                -1  1   7375360  models.common.Conv                      [640, 1280, 3, 2]             \n",
      "  8                -1  4  19676160  models.common.C3                        [1280, 1280, 4]               \n",
      "  9                -1  1   4099840  models.common.SPPF                      [1280, 1280, 5]               \n",
      " 10                -1  1    820480  models.common.Conv                      [1280, 640, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  4   5332480  models.common.C3                        [1280, 640, 4, False]         \n",
      " 14                -1  1    205440  models.common.Conv                      [640, 320, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  4   1335040  models.common.C3                        [640, 320, 4, False]          \n",
      " 18                -1  1    922240  models.common.Conv                      [320, 320, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  4   4922880  models.common.C3                        [640, 640, 4, False]          \n",
      " 21                -1  1   3687680  models.common.Conv                      [640, 640, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  4  19676160  models.common.C3                        [1280, 1280, 4, False]        \n",
      " 24      [17, 20, 23]  1     40374  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [320, 640, 1280]]\n",
      "Model summary: 445 layers, 86217814 parameters, 86217814 gradients, 204.6 GFLOPs\n",
      "\n",
      "Transferred 739/745 items from yolov5x.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 123 weight(decay=0.0), 126 weight(decay=0.0005), 126 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/app/data/train/v8/labels/train' images and labels...136 found,\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /app/data/train/v8/labels/train.cache\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.0GB ram): 100%|██████████| 136/136 [00:00<00:00, 240.95\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/app/data/train/v8/labels/val' images and labels...35 found, 0 mi\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /app/data/train/v8/labels/val.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.0GB ram): 100%|██████████| 35/35 [00:00<00:00, 173.90it/s\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.83 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
      "Plotting labels to /app/train_logs/yolov5x_dv8_tmp/labels.jpg... \n",
      "Image sizes 320 train, 320 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1m/app/train_logs/yolov5x_dv8_tmp\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       0/99      2.21G    0.09469    0.01397          0          3        320: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         35         58    0.00019     0.0345    0.00022   4.66e-05\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "       1/99      2.21G    0.08315    0.01469          0          7        320: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all         35         58   0.000286     0.0517   0.000151    2.5e-05\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "! bash /app/scripts/train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "baseline.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
