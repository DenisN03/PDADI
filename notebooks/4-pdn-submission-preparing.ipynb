{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14e7bed8-1263-4b98-a109-b50b2f4f8c16",
   "metadata": {},
   "source": [
    "# –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ—à–µ–Ω–∏—è\n",
    "–î–∞–Ω–Ω—ã–π –Ω–æ—É—Ç–±—É–∫ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç—å—é –∏ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Ç–æ–≥–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaccd13-9d47-44d7-aa8e-59d7af1f4579",
   "metadata": {},
   "source": [
    "### –û–±—ä—è–≤–ª–µ–Ω–∏–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbf398ae-a0bf-44d9-887e-d3ceb6003e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_predict(markup, image, ROWS, COLS, h_step, w_step, variant, save):\n",
    "    for i, rect in enumerate(crop_submission(ROWS, COLS, h_step, w_step, variant)):\n",
    "        y_0, y_1, x_0, x_1 = rect\n",
    "        cimage = copy.deepcopy(image[y_0:y_1, x_0:x_1])\n",
    "\n",
    "        results = model(cimage, size=640, augment=False)\n",
    "\n",
    "        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–π\n",
    "        for det in results.pandas().xyxy[0].iterrows():\n",
    "            xmin, ymin, xmax, ymax, confidence, clsnum, name = det[1].values\n",
    "            if confidence >= conf:\n",
    "                # print(confidence)\n",
    "                markup.append([xmin + x_0, ymin + y_0, xmax + x_0, ymax + y_0, confidence])\n",
    "                cv2.rectangle(cimage,(int(xmin), int(ymin)),(int(xmax), int(ymax)),(0,255,0), 3)\n",
    "        if save:\n",
    "            cv2.imwrite(f'{yolo_experiment}images/{filename}_{i}_{variant}.jpg', cimage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63849e88-77b1-4d68-8fad-fc92a1366668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image, markup, cv_flip, y_flip, x_flip):\n",
    "    \n",
    "    image_tr = image.copy()\n",
    "    markup_tr = []\n",
    "    \n",
    "    if y_flip or x_flip:\n",
    "        image_tr = cv2.flip(image_tr, cv_flip)\n",
    "\n",
    "    # –¶–∏–∫–ª —Ä–∞–∑–±–∏–µ–Ω–∏—è –∫–∞—Ä—Ç–∏–Ω–∫–∏\n",
    "    crop_predict(markup_tr, image_tr, ROWS, COLS, h_step, w_step, \"B\", save)\n",
    "\n",
    "    if COLS >= 2:\n",
    "        crop_predict(markup_tr, image_tr, ROWS, COLS-1, h_step, w_step, \"C\", save)\n",
    "\n",
    "    if ROWS >= 2:\n",
    "        crop_predict(markup_tr, image_tr, ROWS-1, COLS, h_step, w_step, \"R\", save)\n",
    "\n",
    "    # image = cv2.flip(image, 0)\n",
    "    if (y_flip or x_flip) and len(markup_tr) > 0:\n",
    "        markup_tr = flip_bbox(np.array(markup_tr), (height, width), y_flip=y_flip, x_flip=x_flip)\n",
    "        markup_tr = markup_tr.tolist()\n",
    "        \n",
    "    for mark in markup_tr:\n",
    "        markup.append(mark)\n",
    "\n",
    "    return markup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08171b5-8eca-46f8-9b7d-fa0ca05d4030",
   "metadata": {},
   "source": [
    "### –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –ø—Ä–∏ –ø–æ–º–æ—â–∏ –ù–°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e2072e6-5e98-4d63-8600-ceba81a8736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/archive/master.zip\" to /home/denis/.cache/torch/hub/master.zip\n",
      "YOLOv5 üöÄ 2022-11-6 Python-3.8.10 torch-1.9.1+cu102 CUDA:0 (NVIDIA GeForce GTX 1660 Ti, 5936MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 322 layers, 86173414 parameters, 0 gradients, 203.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3435/3435 [26:02<00:00,  2.20it/s]  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "import sys\n",
    "sys.path.append('/app/src')\n",
    "from transforms import crop_submission\n",
    "from nms import non_max_suppression_fast\n",
    "from markup_convertors import flip_bbox\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import json\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import glob\n",
    "\n",
    "conf = 84\n",
    "nms = 0.30\n",
    "exp_name = f\"_{conf}r_rot_area\"\n",
    "yolo_experiment = \"/app/train_logs/yolov5x_dv7/\"\n",
    "test_images_path = \"/app/data/raw/test_dataset_test/test/\"\n",
    "weights = \"weights/best.pt\"\n",
    "\n",
    "Path(yolo_experiment + \"images\" + exp_name).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=yolo_experiment + weights, force_reload=True)\n",
    "\n",
    "conf = conf / 100\n",
    "\n",
    "df = pd.DataFrame(columns=['ID_img', 'region_shape'])\n",
    "\n",
    "for img_path in tqdm.tqdm(glob.glob(test_images_path + \"*\")):\n",
    "\n",
    "    image = cv2.imread(img_path)\n",
    "    filename = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "    COLS = 4\n",
    "    ROWS = 3\n",
    "\n",
    "    height, width, ch = image.shape\n",
    "\n",
    "    COLS = max(int(width / 1200),1)\n",
    "    ROWS = max(int(height / 1000),1)\n",
    "\n",
    "    w_step = width / COLS\n",
    "    h_step = height / ROWS\n",
    "    \n",
    "    area = width * height\n",
    "\n",
    "    save = False\n",
    "\n",
    "    markup = []\n",
    "    \n",
    "    markup = transform(image, markup, 0, False, False)\n",
    "    markup = transform(image, markup, 0, True, False)\n",
    "    markup = transform(image, markup, 1, False, True)\n",
    "    markup = transform(image, markup, -1, True, True)\n",
    "\n",
    "    # NMS\n",
    "    markup_nms = non_max_suppression_fast(np.array(markup), nms)\n",
    "\n",
    "    if len(markup_nms) == 0:\n",
    "        df = df.append({'ID_img':os.path.basename(img_path), 'region_shape': 0}, ignore_index=True)\n",
    "\n",
    "    else:\n",
    "        shapes = []\n",
    "        for bbox in markup_nms:\n",
    "            width = (bbox[3] - bbox[1]) / 2\n",
    "            height = (bbox[2] - bbox[0]) / 2\n",
    "            r = int(( width + height ) /2 ) * 2\n",
    "            y = int(bbox[1] + width)\n",
    "            x = int(bbox[0] + height)\n",
    "            if r**2 * 100 / area < 0.18:\n",
    "                cv2.circle(image, (x,y), radius=r, color=(0, 0, 255), thickness=3) # –û—Ç—Ä–∏—Å–æ–≤–∫–∞ –¥–µ—Ç–µ–∫—Ü–∏–π –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏\n",
    "                cv2.putText(image, str(np.round(bbox[4],3)),(x,y), cv2.FONT_HERSHEY_PLAIN, 4, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "                shapes.append(str(json.dumps({\"cx\":x,\"cy\":y,\"r\":r})))\n",
    "        \n",
    "        df = df.append({'ID_img':os.path.basename(img_path), 'region_shape': sorted(shapes)}, ignore_index=True)\n",
    "\n",
    "        cv2.imwrite(f'{yolo_experiment}images{exp_name}/{filename}_all.jpg', image)\n",
    "        \n",
    "df.to_csv(f'{yolo_experiment}submission_{yolo_experiment.split(\"/\")[-2]}{exp_name}.csv', index=False, sep=\",\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74fba39-22eb-484c-b38d-e5ba3db928af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
